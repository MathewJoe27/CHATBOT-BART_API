{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chatbot using Bard API"],"metadata":{"id":"ib1AG4FukFEA"}},{"cell_type":"markdown","source":["## Setup Environment"],"metadata":{"id":"OaGoxClLXoP6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kK_XBJ1XHGh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843502183,"user_tz":-330,"elapsed":3316,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"5b84c75e-a1a4-40e6-9877-e2233464daaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bardapi in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->bardapi) (0.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bardapi) (2.32.3)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from bardapi) (0.4.6)\n","Requirement already satisfied: python-gemini-api in /usr/local/lib/python3.11/dist-packages (from bardapi) (2.4.12)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (0.14.0)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->bardapi) (4.2.0)\n","Requirement already satisfied: browser-cookie3 in /usr/local/lib/python3.11/dist-packages (from python-gemini-api->bardapi) (0.20.1)\n","Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from python-gemini-api->bardapi) (0.7.3)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from python-gemini-api->bardapi) (2.10.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from python-gemini-api->bardapi) (3.11.12)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bardapi) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bardapi) (2.3.0)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->bardapi) (4.1.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->python-gemini-api->bardapi) (1.18.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->bardapi) (1.3.1)\n","Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (from browser-cookie3->python-gemini-api->bardapi) (4.4.3)\n","Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.11/dist-packages (from browser-cookie3->python-gemini-api->bardapi) (3.21.0)\n","Requirement already satisfied: jeepney in /usr/lib/python3/dist-packages (from browser-cookie3->python-gemini-api->bardapi) (0.7.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->python-gemini-api->bardapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->python-gemini-api->bardapi) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->python-gemini-api->bardapi) (4.12.2)\n"]}],"source":["!pip install bardapi"]},{"cell_type":"code","source":["from bardapi import Bard\n","import os\n","import time"],"metadata":{"id":"VF1KrU4urUJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace XXXX with the values you get from __Secure-1PSID\n","os.environ['_BARD_API_KEY']=\"XXXX\""],"metadata":{"id":"-rY42uSnXbOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simplest Chatbot"],"metadata":{"id":"SQW9eevUj8ly"}},{"cell_type":"code","source":["input_text = \"What happened in los angeles 2025\"\n","print(Bard().get_answer(input_text)['content'])"],"metadata":{"id":"HIAWUQNPvD97","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843272073,"user_tz":-330,"elapsed":6774,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"e95087e4-3815-4088-aa27-61418e367608"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025 has been a busy year for Los Angeles, with a mix of exciting events and some challenges. Here's a quick overview:\n","\n","**Major Events:**\n","\n","*   **Wildfires:** Unfortunately, Southern California experienced a series of destructive wildfires in January 2025. These fires affected Los Angeles and San Diego counties, causing significant damage and displacement.\n","*   **Festivals and Cultural Celebrations:** Los Angeles hosted a wide range of popular events, including:\n","    *   Coachella Music Festival in April\n","    *   Rolling Loud music festival in March\n","    *   The Golden Dragon Parade in February\n","    *   LA Pride in June\n","    *   Anime Expo in July\n","*   **Awards Shows:** As usual, Los Angeles was the center of the awards season, hosting prestigious events like the Golden Globes, the Grammy Awards, and the Academy Awards.\n","*   **Sports:** Los Angeles continued to be a hub for sports, with events like the LA Marathon and various games featuring the city's professional teams.\n","\n","**Other Notable Occurrences:**\n","\n","*   **Drought:** Southern California faced severe drought conditions, which contributed to the heightened risk of wildfires.\n","*   **La Niña:** The El Niño-Southern Oscillation shifted to La Niña, which influenced weather patterns and potentially exacerbated the drought.\n","\n","Overall, 2025 in Los Angeles was a year of contrasts, with both celebrations and challenges. The city showcased its vibrant culture and entertainment scene, but also faced natural disasters and environmental concerns.\n","\n"]}]},{"cell_type":"markdown","source":["## Custom Chatbot"],"metadata":{"id":"8NU1b3f5kdkc"}},{"cell_type":"markdown","source":["### Text Summarizer"],"metadata":{"id":"x0pzfK-gkzJU"}},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6krlXbIVwsZe","executionInfo":{"status":"ok","timestamp":1739843282592,"user_tz":-330,"elapsed":3564,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"f6211f5f-d30a-4b6d-a306-e60e6e0128e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n","Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/300.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m245.8/300.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-5.3.0\n"]}]},{"cell_type":"code","source":["from pypdf import PdfReader"],"metadata":{"id":"P6-zH35Dw0Xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ptCfCdRprdOH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843305109,"user_tz":-330,"elapsed":22526,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"c62dba16-e6d0-41ca-e3f2-f2c84733e2f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/CHATBOT/'\n","filename  = 'Attention-Is-All-You-Need.pdf'\n","# filename  = 'Art-of-War.pdf'"],"metadata":{"id":"n6d42reCXq7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a pdf file object\n","pdfFileObject = open(directory+filename, 'rb')\n","# creating a pdf reader object\n","pdfReader = PdfReader(pdfFileObject)\n","text=[]\n","summary=' '\n","#Storing the pages in a list\n","for i in range(0,len(pdfReader.pages)):\n","  # creating a page object\n","  pageObj = pdfReader.pages[i].extract_text()\n","  pageObj= pageObj.replace('\\t\\r','')\n","  pageObj= pageObj.replace('\\xa0','')\n","  # extracting text from page\n","  text.append(pageObj)"],"metadata":{"id":"gYpNlLqcZVyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge multiple page - to reduce API Calls\n","def join_elements(lst, chars_per_element):\n","    new_lst = []\n","    for i in range(0, len(lst), chars_per_element):\n","        new_lst.append(''.join(lst[i:i+chars_per_element]))\n","    return new_lst\n","\n","# Option to keep x elements per list element\n","new_text = join_elements(text, 3)\n","\n","print(f\"Original Pages = \",len(text))\n","print(f\"Compressed Pages = \",len(new_text))"],"metadata":{"id":"0y8iM7trjKdM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843353353,"user_tz":-330,"elapsed":373,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"69bc5ba2-4741-4454-948a-b443269e0c24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Pages =  15\n","Compressed Pages =  5\n"]}]},{"cell_type":"code","source":["def get_completion(prompt):\n","  response = Bard().get_answer(prompt)['content']\n","  return response"],"metadata":{"id":"AJ_lIoBzX267"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(new_text)):\n","  prompt =f\"\"\"\n","  Your task is to act as a Text Summariser.\n","  I'll give you text from  pages of a book from beginning to end.\n","  And your job is to summarise text from these pages in less than 100 words.\n","  Don't be conversational. I need a plain 100 word answer.\n","  Text is shared below, delimited with triple backticks:\n","  ```{text[i]}```\n","  \"\"\"\n","  try:\n","    response = get_completion(prompt)\n","  except:\n","    response = get_completion(prompt)\n","  print(response)\n","  summary= summary+' ' +response +'\\n\\n'\n","  # result.append(response)\n","  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay"],"metadata":{"id":"CxpOAcNuZV3y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843463694,"user_tz":-330,"elapsed":105115,"user":{"displayName":"Mathew Joe","userId":"03075942300758342909"}},"outputId":"4d3ffb62-ad49-4cd6-cbcb-49c3bab78408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks like machine translation.  Unlike previous models relying on recurrent or convolutional networks, the Transformer solely uses attention mechanisms, eliminating recurrence and convolutions.  This leads to increased parallelization and significantly faster training.  The Transformer achieves state-of-the-art results on WMT 2014 English-to-German and English-to-French translation tasks, surpassing existing models by over 2 BLEU points.  It also generalizes well to other tasks, demonstrated by its successful application to English constituency parsing.  The authors highlight the contributions of each researcher involved in the project.\n","\n","Recurrent models, while effective in sequence modeling, suffer from sequential computation limitations, hindering parallelization, especially with long sequences.  Attention mechanisms address this by modeling dependencies regardless of distance, but are often used with recurrent networks. This work introduces the Transformer, a novel architecture that eschews recurrence entirely, relying solely on attention.  Unlike convolutional approaches like ByteNet and ConvS2S, which have increasing computational complexity with distance, the Transformer maintains constant complexity.  While self-attention has been used in various tasks, the Transformer is the first transduction model to exclusively use it, eliminating the need for RNNs or convolutions for input/output representation computation.  The Transformer employs an encoder-decoder structure with stacked self-attention and fully connected layers.\n","\n","The Transformer model uses stacked encoders and decoders.  Each encoder layer has self-attention and a feed-forward network, with residual connections and layer normalization.  The decoder has these plus a third layer performing multi-head attention over the encoder output.  Decoder self-attention is masked to prevent attending to future positions.  All sub-layers and embeddings have dimension dmodel=512. Attention is calculated by mapping a query and key-value pairs to an output. The output is a weighted sum of the values, weighted by the compatibility of the query and key.\n","\n","Scaled Dot-Product Attention computes attention weights by taking the dot product of queries and keys, scaling by the square root of the key dimension (dk), and applying a softmax function.  This scaling is crucial for larger dk values to prevent vanishing gradients.  The attention is calculated efficiently using matrix multiplication. Multi-Head Attention improves upon this by linearly projecting the queries, keys, and values multiple times before performing the scaled dot-product attention in parallel. The results are then concatenated and projected again, creating a more robust and expressive attention mechanism.\n","\n","Multi-head attention allows the model to attend to information from different representation subspaces.  It uses multiple parallel attention layers (heads) with reduced dimensionality, maintaining computational cost. The Transformer employs multi-head attention in three ways: encoder-decoder attention (decoder attends to encoder output), encoder self-attention (encoder attends to its previous layer's output), and decoder self-attention (decoder attends to its previous output, masked to prevent future information access).  Each encoder and decoder layer also contains a position-wise feed-forward network with two linear transformations and a ReLU activation.  Learned embeddings convert input/output tokens to vectors.  Positional encodings are added to capture sequence order since the model lacks recurrence and convolution.\n","\n"]}]},{"cell_type":"code","source":["with open(directory +'/bard_summary.txt',\n","          'w') as out:\n","  out.write(summary)"],"metadata":{"id":"6r8WE86PupBV"},"execution_count":null,"outputs":[]}]}